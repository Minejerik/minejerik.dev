TITLE:A first look at MNNlibv2
SOURCE:https://github.com/Minejerik/mnnlibv2
DATE:October 18th 2023


![screenshot showing how simple MNNlibv2](/static/3/top.png)
###### Look at the simplcity of MNNlibv2

Hello again! I have been working on some new stuff, this time involving neural networks!    
I present MNNlibv2, the second generation of my neural network libraries.    
I have designed MNNlib to be as simple as possible, allowing even the dumbest of programmers to make neural networks.    

## The Design
Everything is a class, this is the design principle of MNNlib. 
This focus on classes allows the programmer to extend them as they wish.
If you want to add some advanced features to a network, just inherit from the `network` class. 
Want to make a custom layer or neuron class? just extend those classes!

## Current Issues
I will admit that this is not as feature rich as `tensorflow` or `pytorch` but it is easier for the average person.
One of the biggest issues with MNNlib is the lack of true training. 
Right now the training works by getting all of the `weight` classes and picking one and setting the weight at random.
This works but it is very innefficent.
I am working on implementing much better training algorithms, such as, `gradient descent`.

## Example
Here is some example code that makes the network emulate an `and` gate.

        from mnn import network, layer, activations as a, dataset, trainer
        
        
        #create network
        net = network()
        #create input layer w/ relu activation function
        #it has 2 input and output nodes
        lay = layer(2,2,a.relu)
        net.add_layer(lay)
        #create hidden layer also w/ relu activation function
        #this time w/ 2 input nodes and 4 output nodes
        lay = layer(2,4,a.relu)
        net.add_layer(lay)
        #create output layer w/ straight activation function
        #it has 4 input nodes and 1 output node
        lay = layer(4,1,a.straight)
        net.add_layer(lay)
        
        #print the network
        #allows for easy viewing
        print(net)
        
        #create dataset
        data = dataset()
        
        
        #add some data to it
        #this emulates an and gate
        data.add_data([0,0],[0])
        data.add_data([0,1],[1])
        data.add_data([1,0],[1])
        data.add_data([1,1],[1])
        
        
        #create trainer
        #this trains the network in a later step
        train = trainer(net,75,data)
        
        #print network loss before training
        print(train.get_full_data_loss())
        
        #start training
        train.start_train()
        
        #print network loss after training
        print(train.get_full_data_loss())
        
        #get the network
        net = train.get_net()
        
        
        #print the network's output from all the data
        print(net.run_all_data(data))

## Thank you for reading
Thank you for reading this, I know it is shorter then my other posts but I wanted to get it out quickly!
I will continue working on MNNlib until it works well.